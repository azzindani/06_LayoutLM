version: '3.8'

services:
  # Gradio UI service
  ui:
    build: .
    container_name: layoutlm-ui
    ports:
      - "7860:7860"
    environment:
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      - MODEL_NAME=nielsr/layoutlmv3-finetuned-funsd
      - DEVICE=auto
      - LOG_LEVEL=INFO
    volumes:
      - model-cache:/app/models
    command: ["python", "main.py", "--mode", "ui"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # API service
  api:
    build: .
    container_name: layoutlm-api
    ports:
      - "8000:8000"
    environment:
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - MODEL_NAME=nielsr/layoutlmv3-finetuned-funsd
      - DEVICE=auto
      - LOG_LEVEL=INFO
    volumes:
      - model-cache:/app/models
    command: ["python", "main.py", "--mode", "api"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Combined service (both UI and API)
  all:
    build: .
    container_name: layoutlm-all
    ports:
      - "7860:7860"
      - "8000:8000"
    environment:
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - MODEL_NAME=nielsr/layoutlmv3-finetuned-funsd
      - DEVICE=auto
      - LOG_LEVEL=INFO
    volumes:
      - model-cache:/app/models
    command: ["python", "main.py", "--mode", "all"]
    profiles:
      - full
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  model-cache:
    name: layoutlm-models
